{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    target_col = 'pm2_5'\n",
    "    n_splits = 4\n",
    "    random_state = 42\n",
    "    id_col = 'id'\n",
    "    missing_threshold = 0.7\n",
    "    top_features = 70\n",
    "    clip_threshold = 0.97\n",
    "    corr_threshold = 0.9\n",
    "\n",
    "    # Default hyperparameters\n",
    "    cat_params = {\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': 0.03,\n",
    "        'depth': 6,\n",
    "        'eval_metric': 'RMSE',\n",
    "        'random_seed': random_state,\n",
    "        'early_stopping_rounds': 250,\n",
    "        'verbose': 100\n",
    "    }\n",
    "    lgb_params = {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': -1,\n",
    "        'random_state': random_state,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "    xgb_params = {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.3,\n",
    "        'max_depth': 6,\n",
    "        'random_state': random_state,\n",
    "        'objective': 'reg:squarederror'\n",
    "    }\n",
    "    lasso_params = {'alpha': 0.001, 'random_state': random_state}\n",
    "    svr_params = {'C': 1.0, 'epsilon': 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('dataset/Train.csv')\n",
    "test = pd.read_csv('dataset/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with too many missing values\n",
    "train = train.loc[:, train.isnull().mean() < Config.missing_threshold]\n",
    "test = test.loc[:, test.isnull().mean() < Config.missing_threshold]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GroupKFold\n",
    "def create_folds(data):\n",
    "    data['folds'] = -1\n",
    "    gkf = GroupKFold(n_splits=Config.n_splits)\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X=data, groups=data['city']), start=1):\n",
    "        data.loc[val_idx, 'folds'] = fold\n",
    "    return data\n",
    "\n",
    "train = create_folds(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PM2.5 standard deviation per location\n",
    "location_variance = train.groupby('city')[Config.target_col].std().reset_index()\n",
    "location_variance.columns = ['city', 'pm2_5_std']\n",
    "location_variance = location_variance.sort_values(by='pm2_5_std', ascending=False).reset_index(drop=True)\n",
    "location_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in the target variable\n",
    "plt.figure(figsize = (22, 10))\n",
    "sns.boxplot(train.pm2_5)\n",
    "plt.title('Boxplot showing outliers - target variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations between numerical features and target\n",
    "train_num_df = train.select_dtypes(include=['number'])\n",
    "top10_corrs = abs(train_num_df.corr()['pm2_5']).sort_values(ascending=False).head(10)\n",
    "corr = train_num_df[list(top10_corrs.index)].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8), dpi=150)\n",
    "sns.heatmap(\n",
    "    corr, cmap='RdYlGn', annot=True, center=0, fmt=\".2f\", \n",
    "    annot_kws={\"size\": 9}\n",
    ")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Correlations between the target and other numeric variables', pad=15, fontdict={'size': 14})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def feature_engineering(train, test):\n",
    "    le = LabelEncoder()\n",
    "    data = pd.concat([train, test])\n",
    "    data['location'] = data['site_latitude'].astype('str') + '_' + data['site_longitude'].astype('str')\n",
    "    data = data.sort_values(by = ['city','location', 'date', 'hour'])\n",
    "    categorical_cols = data.select_dtypes(include='object').columns.tolist()\n",
    "    categorical_cols = [col for col in categorical_cols if col not in ['date', 'id', 'city', 'country']]\n",
    "    print(f'Categorical columns: {categorical_cols}')\n",
    "\n",
    "    # Date features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['week'] = data['date'].dt.isocalendar().week\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['dayofweek'] = data['date'].dt.dayofweek\n",
    "    data['is_weekend'] = data['dayofweek'].isin([5,6]).astype(int)\n",
    "\n",
    "    numerical_cols = data.select_dtypes(exclude='object').columns.tolist()\n",
    "    numerical_cols.remove(Config.target_col)\n",
    "    numerical_cols.remove('folds')\n",
    "    numerical_cols.remove('hour')\n",
    "    numerical_cols.remove('site_latitude')\n",
    "    numerical_cols.remove('site_longitude') \n",
    "    print(f'Numerical columns: {numerical_cols}')\n",
    "\n",
    "    # Fill in missing values by forward and backward fill within each city and location\n",
    "    nan_cols = [col for col in numerical_cols if data[col].isnull().sum() > 0 and col not in [Config.target_col, \"folds\"]]\n",
    "    for col in nan_cols:\n",
    "        data[col] = (\n",
    "            data.groupby([\"city\", \"location\"])[col]\n",
    "                .transform(lambda x: x.ffill().bfill())\n",
    "                .fillna(data[col].median())  # global fallback\n",
    "            )\n",
    "\n",
    "    # Encode categorical features\n",
    "    for col in categorical_cols + ['date']:\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "\n",
    "    # Split back into train and test\n",
    "    train  = data[data['id'].isin(train['id'].unique())]\n",
    "    test = data[data['id'].isin(test['id'].unique())]\n",
    "\n",
    "    features = [col for col in data.columns if col not in \n",
    "                [Config.target_col, Config.id_col, 'folds', 'country', 'city', 'site_id', 'site_latitude', 'site_longitude']]\n",
    "  \n",
    "    return train, test, features\n",
    "\n",
    "train, test, features = feature_engineering(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_feature_selection(X, y, k=Config.top_features):\n",
    "    \"\"\"\n",
    "    Select top-k features based on CatBoost feature importance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix\n",
    "    y : pd.Series or np.ndarray\n",
    "        Target vector\n",
    "    k : int\n",
    "        Number of top features to select\n",
    "    \"\"\"\n",
    "    model = CatBoostRegressor(**Config.cat_params)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    feature_importances = model.get_feature_importance(prettified=True)\n",
    "    top_features = feature_importances.head(k)['Feature Id'].tolist()\n",
    "\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated features\n",
    "def drop_highly_correlated_features(X, threshold=0.9):\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "    print(f'Dropping {len(to_drop)} highly correlated features: {to_drop}')\n",
    "    reduced_features = [feature for feature in X.columns.tolist() if feature not in to_drop]\n",
    "    return reduced_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self, top_features=30, corr_threshold=0.9, clip_threshold=0.99):\n",
    "        self.top_features = top_features\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.clip_threshold = clip_threshold\n",
    "        self.models = None\n",
    "        self.reduced_features = None\n",
    "        self.best_params = None\n",
    "\n",
    "    def _clip_target(self, y):\n",
    "        clip_val = np.quantile(y, self.clip_threshold)\n",
    "        return np.where(y >= clip_val, clip_val, y)\n",
    "\n",
    "    def _feature_selection(self, X, y):\n",
    "        top_feats = top_k_feature_selection(X, y, k=self.top_features)\n",
    "        self.reduced_features = drop_highly_correlated_features(X[top_feats], threshold=self.corr_threshold)\n",
    "\n",
    "    def _fit_models(self, X, y, models):\n",
    "        for name, model in models.items():\n",
    "            model.fit(X[self.reduced_features], y)\n",
    "        return models\n",
    "\n",
    "    def cross_val_fit(self, X, y, folds, models, X_test=None):\n",
    "        \"\"\"Run CV to evaluate hyperparameters; returns OOF, test predictions, mean RMSE.\"\"\"\n",
    "        oof_preds = np.zeros(len(y))\n",
    "        fold_test_preds = [] if X_test is not None else None\n",
    "        fold_rmse_list = []\n",
    "\n",
    "        unique_folds = np.unique(folds)\n",
    "        self._feature_selection(X, y)\n",
    "\n",
    "        for fold in unique_folds:\n",
    "            train_idx = folds != fold\n",
    "            val_idx = folds == fold\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "            y_train_clipped = self._clip_target(y_train)\n",
    "\n",
    "            val_preds = np.zeros((len(X_val), len(models)))\n",
    "            test_preds = np.zeros((len(X_test), len(models))) if X_test is not None else None\n",
    "\n",
    "            # Train each model\n",
    "            models_fold = self._fit_models(X_train, y_train_clipped, models)\n",
    "            for i, (name, model) in enumerate(models_fold.items()):\n",
    "                val_preds[:, i] = model.predict(X_val[self.reduced_features])\n",
    "                if X_test is not None:\n",
    "                    test_preds[:, i] = model.predict(X_test[self.reduced_features])\n",
    "\n",
    "            # Ensemble\n",
    "            oof_preds[val_idx] = val_preds.mean(axis=1)\n",
    "            if X_test is not None:\n",
    "                fold_test_preds.append(test_preds.mean(axis=1))\n",
    "\n",
    "            # Fold RMSE\n",
    "            fold_rmse = np.sqrt(np.mean((y_val - oof_preds[val_idx])**2))\n",
    "            fold_rmse_list.append(fold_rmse)\n",
    "\n",
    "        mean_rmse = np.mean(fold_rmse_list)\n",
    "        final_test_preds = np.mean(fold_test_preds, axis=0) if X_test is not None else None\n",
    "        return oof_preds, final_test_preds, mean_rmse\n",
    "\n",
    "    def fit_final(self, X, y, models):\n",
    "        \"\"\"Train final ensemble on full training data after hyperparameter selection.\"\"\"\n",
    "        y_clipped = self._clip_target(y)\n",
    "        self._feature_selection(X, y_clipped)\n",
    "        self.models = self._fit_models(X, y_clipped, models)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the ensemble (mean of all models).\"\"\"\n",
    "        test_preds = np.zeros((len(X), len(self.models)))\n",
    "        for i, (name, model) in enumerate(self.models.items()):\n",
    "            test_preds[:, i] = model.predict(X[self.reduced_features])\n",
    "        return np.mean(test_preds, axis=1)\n",
    "\n",
    "ensemble = EnsembleModel(top_features=Config.top_features,\n",
    "                         corr_threshold=Config.corr_threshold,\n",
    "                         clip_threshold=Config.clip_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation with GroupKFold for hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    models = {\n",
    "        \"cat\": CatBoostRegressor(\n",
    "            iterations=1000,\n",
    "            learning_rate=trial.suggest_float('cat_lr', 0.01, 0.1, log=True),\n",
    "            depth=trial.suggest_int('cat_depth', 4, 10),\n",
    "            random_seed=Config.random_state,\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=250\n",
    "        ),\n",
    "        \"lgb\": LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=trial.suggest_float('lgb_lr', 0.01, 0.1, log=True),\n",
    "            max_depth=trial.suggest_int('lgb_depth', 3, 12),\n",
    "            random_state=Config.random_state,\n",
    "            verbosity=-1\n",
    "        ),\n",
    "        \"xgb\": XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=trial.suggest_float('xgb_lr', 0.01, 0.1, log=True),\n",
    "            max_depth=trial.suggest_int('xgb_depth', 3, 12),\n",
    "            random_state=Config.random_state,\n",
    "            objective='reg:squarederror'\n",
    "        ),\n",
    "        \"lasso\": Lasso(alpha=trial.suggest_float('lasso_alpha', 1e-4, 1.0, log=True),\n",
    "                       random_state=Config.random_state),\n",
    "        \"svr\": SVR(\n",
    "            C=trial.suggest_float('svr_C', 0.1, 10.0, log=True),\n",
    "            epsilon=trial.suggest_float('svr_eps', 0.01, 1.0, log=True)\n",
    "        )\n",
    "    }\n",
    "\n",
    "    _, _, mean_rmse = ensemble.cross_val_fit(\n",
    "        train[features],\n",
    "        train[Config.target_col].values,\n",
    "        folds=train['folds'].values,\n",
    "        models=models\n",
    "    )\n",
    "        \n",
    "    return mean_rmse\n",
    "\n",
    "# Optimize hyperparameters\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final train on full data and predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best hyperparameters\n",
    "best_params = study.best_params\n",
    "Config.cat_params['learning_rate'] = best_params['cat_lr']\n",
    "Config.cat_params['depth'] = best_params['cat_depth']\n",
    "Config.lgb_params['learning_rate'] = best_params['lgb_lr']\n",
    "Config.lgb_params['max_depth'] = best_params['lgb_depth']\n",
    "Config.xgb_params['learning_rate'] = best_params['xgb_lr']   \n",
    "Config.xgb_params['max_depth'] = best_params['xgb_depth']\n",
    "Config.lasso_params['alpha'] = best_params['lasso_alpha']\n",
    "Config.svr_params['C'] = best_params['svr_C']\n",
    "Config.svr_params['epsilon'] = best_params['svr_eps']\n",
    "\n",
    "# Define final models with best hyperparameters\n",
    "best_models = {\n",
    "    \"cat\": CatBoostRegressor(**Config.cat_params),\n",
    "    \"lgb\": LGBMRegressor(**Config.lgb_params),\n",
    "    \"xgb\": XGBRegressor(**Config.xgb_params),\n",
    "    \"lasso\": Lasso(**Config.lasso_params),\n",
    "    \"svr\": SVR(**Config.svr_params)\n",
    "}\n",
    "\n",
    "ensemble.fit_final(train[features], train[Config.target_col].values, best_models)\n",
    "final_test_preds = ensemble.predict(test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best models\n",
    "folder_name = f\"./weights/{study.best_value:.2f}\"  # keep 4 decimal places\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "for name, model in best_models.items():\n",
    "    joblib.dump(model, f\"{folder_name}/best_model_{name}.pkl\")\n",
    "\n",
    "# Save reduced feature list\n",
    "with open(f\"{folder_name}/reduced_features.json\", \"w\") as f:\n",
    "    json.dump(ensemble.reduced_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance from final CatBoost model\n",
    "plt.figure(figsize=(20, 16))\n",
    "feature_importances_df = pd.DataFrame(best_models['cat'].feature_importances_, columns=['Importances'])\n",
    "feature_importances_df['Feature'] = ensemble.reduced_features\n",
    "sns.barplot(x='Importances', y='Feature', data=feature_importances_df.sort_values(by=['Importances'], ascending=False).head(20))\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submission(test, best_test_pred, save_path='output/submission.csv'):\n",
    "    \"\"\"Prepare submission file.\"\"\"\n",
    "    test[Config.target_col] = best_test_pred\n",
    "    submission = test[[Config.id_col, Config.target_col]]\n",
    "    submission.to_csv(save_path, index=False)\n",
    "    submission.head()\n",
    "    \n",
    "prepare_submission(test, final_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best models and save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_predict(test, rmse=27.84, model_names=None):\n",
    "    \"\"\"\n",
    "    Load best models, make predictions on test set, and ensemble them.\n",
    "\n",
    "    Args:\n",
    "        test (pd.DataFrame): Test dataset.\n",
    "        rmse (float): RMSE value to locate model folder.\n",
    "        model_names (list): List of model names to load. Defaults to ['cat', 'lgb', 'xgb', 'lasso', 'svr'].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Ensemble predictions for the test set.\n",
    "    \"\"\"\n",
    "    if model_names is None:\n",
    "        model_names = ['cat', 'lgb', 'xgb', 'lasso', 'svr']\n",
    "\n",
    "    # Load models\n",
    "    folder_name = f\"./weights/{rmse:.2f}\"\n",
    "    models = {name: joblib.load(f\"{folder_name}/best_model_{name}.pkl\") for name in model_names}\n",
    "\n",
    "    # Load reduced features\n",
    "    with open(f\"{folder_name}/reduced_features.json\", \"r\") as f:\n",
    "        features = json.load(f)\n",
    "\n",
    "    # Prepare test predictions\n",
    "    test_preds = np.zeros((len(test), len(models)))\n",
    "\n",
    "    # Predict\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        test_preds[:, i] = model.predict(test[features])\n",
    "\n",
    "    # Ensemble with equal weights\n",
    "    final_test_pred = np.mean(test_preds, axis=1)\n",
    "\n",
    "    return final_test_pred\n",
    "\n",
    "final_test_preds = load_and_predict(test, rmse=27.78)\n",
    "\n",
    "# Save final submission\n",
    "# prepare_submission(test, final_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing by multiply with a scale factor\n",
    "scale_factor = 1  # Adjust this factor based on validation results\n",
    "final_test_preds_scaled = final_test_preds * scale_factor\n",
    "prepare_submission(test, final_test_preds_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI7101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
